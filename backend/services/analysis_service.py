import os
import json
import io
import asyncio
from loguru import logger

from backend.api.models import PreparationAnalysis, ResultsAnalysis, FullReport
from ..core.config import settings
from backend.utils import file_processing as fp
from backend.agents.pipeline_1_pre_interview.agent_1_data_parser import agent_1_data_parser
from backend.agents.pipeline_1_pre_interview.agent_2_grader import agent_2_grader
from backend.agents.pipeline_1_pre_interview.agent_3_report_generator import agent_3_report_generator
from backend.agents.pipeline_2_post_interview.agent_4_topic_extractor import agent_4_topic_extractor
from backend.agents.pipeline_2_post_interview.agent_5_final_report_generator import agent_5_final_report_generator

import assemblyai as aai
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from google.oauth2 import service_account
from googleapiclient.discovery import build
import httplib2
from google_auth_httplib2 import AuthorizedHttp


class AnalysisService:
    """Service responsible for interview analysis business logic using AI Agents"""

    def __init__(self):
        self.semaphore = asyncio.Semaphore(1)
        if settings.assemblyai_api_key:
            aai.settings.api_key = settings.assemblyai_api_key
            logger.success("–ö–ª–∏–µ–Ω—Ç AssemblyAI —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω.")
        else:
            logger.warning("API –∫–ª—é—á –¥–ª—è AssemblyAI –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")

        self.drive_service = None
        self.request_counter = 0
        self.session_total_tokens = 0
        try:
            credentials_path = settings.google_application_credentials
            if os.path.exists(credentials_path):
                creds = service_account.Credentials.from_service_account_file(credentials_path)
                scoped_credentials = creds.with_scopes(['https://www.googleapis.com/auth/drive'])
                http_client_with_timeout = httplib2.Http(timeout=600)
                authed_http = AuthorizedHttp(scoped_credentials, http=http_client_with_timeout)
                self.drive_service = build(
                    'drive',
                    'v3',
                    http=authed_http,
                    cache_discovery=False
                )

                logger.success("–ö–ª–∏–µ–Ω—Ç Google Drive API —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
            else:
                logger.error(f"–§–∞–π–ª —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö Google –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {credentials_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞ Google Drive API: {e}", exc_info=True)

    def _set_google_api_key(self):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–ª—é—á Google API –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        """
        api_key_to_use = settings.google_api_key
        if not api_key_to_use:
            logger.error("–ö–ª—é—á Google API (google_api_key) –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ.")
            raise ValueError("Google API key is not provided.")
        os.environ['GOOGLE_API_KEY'] = api_key_to_use
        logger.info("–ö–ª—é—á Google API —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")

    async def analyze_preparation(
            self,
            cv_file: io.BytesIO,
            cv_filename: str,
            feedback_text: str,
            requirements_link: str
    ) -> PreparationAnalysis:
        async with self.semaphore:
            logger.info("–ù–∞—á–∞–ª–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–ü–∞–π–ø–ª–∞–π–Ω 1)...")
            pipeline_tokens_used = 0

            self._set_google_api_key()

            cv_text = fp.read_file_content(cv_file, cv_filename)

            requirements_file_id = fp.get_google_drive_file_id(requirements_link)
            requirements_text = await fp.download_sheet_from_drive(self.drive_service, requirements_file_id)

            session_service = InMemorySessionService()
            session_id = f"prep_session_{os.urandom(8).hex()}"
            user_id = "prep_user"
            await session_service.create_session(app_name=settings.app_name, user_id=user_id, session_id=session_id)

            logger.info("üöÄ –ó–∞–ø—É—Å–∫ agent_1_data_parser...")
            runner_1 = Runner(agent=agent_1_data_parser, app_name=settings.app_name, session_service=session_service)
            message_for_agent_1 = types.Content(
                role="user",
                parts=[
                    types.Part(text=f"cv_text: {cv_text}"),
                    types.Part(text=f"requirements_text: {requirements_text}"),
                    types.Part(text=f"feedback_text: {feedback_text}")
                ]
            )
            agent_1_output = ""
            async for event in runner_1.run_async(session_id=session_id, user_id=user_id, new_message=message_for_agent_1):
                if event.usage_metadata:
                    pipeline_tokens_used += event.usage_metadata.total_token_count
                    logger.info(f"–¢–æ–∫–µ–Ω—ã (–ê–≥–µ–Ω—Ç 1): –í—Ö–æ–¥={event.usage_metadata.prompt_token_count}, –í—ã—Ö–æ–¥={event.usage_metadata.candidates_token_count}, –í—Å–µ–≥–æ={event.usage_metadata.total_token_count}")
                if event.content and event.content.parts:
                    agent_1_output += "".join(part.text for part in event.content.parts if part.text)

            logger.info("üöÄ –ó–∞–ø—É—Å–∫ agent_2_grader...")
            runner_2 = Runner(agent=agent_2_grader, app_name=settings.app_name, session_service=session_service)
            message_for_agent_2 = types.Content(role="user", parts=[types.Part(text=agent_1_output)])
            agent_2_output = ""
            async for event in runner_2.run_async(session_id=session_id, user_id=user_id, new_message=message_for_agent_2):
                if event.usage_metadata:
                    pipeline_tokens_used += event.usage_metadata.total_token_count
                    logger.info(f"–¢–æ–∫–µ–Ω—ã (–ê–≥–µ–Ω—Ç 2): –í—Ö–æ–¥={event.usage_metadata.prompt_token_count}, –í—ã—Ö–æ–¥={event.usage_metadata.candidates_token_count}, –í—Å–µ–≥–æ={event.usage_metadata.total_token_count}")
                if event.content and event.content.parts:
                    agent_2_output += "".join(part.text for part in event.content.parts if part.text)

            logger.info("üöÄ –ó–∞–ø—É—Å–∫ agent_3_report_generator...")
            runner_3 = Runner(agent=agent_3_report_generator, app_name=settings.app_name, session_service=session_service)
            message_for_agent_3 = types.Content(role="user", parts=[types.Part(text=agent_2_output)])
            final_output = ""
            async for event in runner_3.run_async(session_id=session_id, user_id=user_id, new_message=message_for_agent_3):
                if event.usage_metadata:
                    pipeline_tokens_used += event.usage_metadata.total_token_count
                    logger.info(f"–¢–æ–∫–µ–Ω—ã (–ê–≥–µ–Ω—Ç 3): –í—Ö–æ–¥={event.usage_metadata.prompt_token_count}, –í—ã—Ö–æ–¥={event.usage_metadata.candidates_token_count}, –í—Å–µ–≥–æ={event.usage_metadata.total_token_count}")
                if event.content and event.content.parts:
                    final_output += "".join(part.text for part in event.content.parts if part.text)

            self.session_total_tokens += pipeline_tokens_used
            logger.info(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –ü–∞–π–ø–ª–∞–π–Ω 1: {pipeline_tokens_used}")
            logger.info(f"–û–±—â–∏–π —Ä–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {self.session_total_tokens}")

            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞...")
            try:
                clean_json_str = final_output.strip().replace("```json", "").replace("```", "").strip()
                data = json.loads(clean_json_str)

                final_response_data = {
                    "message": "Interview preparation report created successfully.",
                    **data
                }
                return PreparationAnalysis(**final_response_data)
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –æ—Ç –ê–≥–µ–Ω—Ç–∞ 3: {e}\n–ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {final_output}")
                raise ValueError("AI-—Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö.")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic –∏–ª–∏ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞: {e}")
                raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {e}")

    async def analyze_results(
            self,
            cv_file: io.BytesIO,
            cv_filename: str,
            video_link: str,
            competency_matrix_link: str,
            department_values_link: str,
            employee_portrait_link: str,
            job_requirements_link: str
    ) -> ResultsAnalysis:
        async with self.semaphore:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ü–∞–π–ø–ª–∞–π–Ω–∞ 2: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–≤—å—é...")
            pipeline_tokens_used = 0
            temp_audio_path = None

            try:
                self._set_google_api_key()

                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ...")
                video_file_id = fp.get_google_drive_file_id(video_link)
                temp_audio_path = await fp.download_audio_from_drive_to_temp_file(self.drive_service, video_file_id)
                transcription_text = await fp.transcribe_audio_assemblyai(temp_audio_path)
                if not transcription_text:
                    raise ValueError("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ç–µ–∫—Å—Ç. –í–∏–¥–µ–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–µ–∑ –∑–≤—É–∫–∞ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º.")

                cv_text = fp.read_file_content(cv_file, cv_filename)

                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏–∑ Google Drive...")
                links = {
                    "matrix": competency_matrix_link,
                    "values": department_values_link,
                    "portrait": employee_portrait_link,
                    "requirements": job_requirements_link,
                }

                drive_data = {}
                for key, link in links.items():
                    file_id = fp.get_google_drive_file_id(link)
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–±–ª–∏—Ü—ã '{key}' —Å ID: {file_id}...")
                    data = await fp.download_sheet_from_drive(self.drive_service, file_id)
                    drive_data[key] = data

                matrix_text = drive_data["matrix"]
                values_text = drive_data["values"]
                portrait_text = drive_data["portrait"]
                requirements_text = drive_data["requirements"]
                session_service = InMemorySessionService()
                session_id = f"results_session_{os.urandom(8).hex()}"
                user_id = "results_user"
                await session_service.create_session(app_name=settings.app_name, user_id=user_id, session_id=session_id)

                logger.info("üöÄ –ó–∞–ø—É—Å–∫ agent_4_topic_extractor...")
                runner_4 = Runner(agent=agent_4_topic_extractor, app_name=settings.app_name, session_service=session_service)
                message_for_agent_4 = types.Content(role="user", parts=[types.Part(text=transcription_text)])
                agent_4_output = ""
                async for event in runner_4.run_async(session_id=session_id, user_id=user_id, new_message=message_for_agent_4):
                    if event.usage_metadata:
                        pipeline_tokens_used += event.usage_metadata.total_token_count
                        logger.info(f"–¢–æ–∫–µ–Ω—ã (–ê–≥–µ–Ω—Ç 4): –í—Ö–æ–¥={event.usage_metadata.prompt_token_count}, –í—ã—Ö–æ–¥={event.usage_metadata.candidates_token_count}, –í—Å–µ–≥–æ={event.usage_metadata.total_token_count}")
                    if event.content and event.content.parts:
                        agent_4_output += "".join(part.text for part in event.content.parts if part.text)

                combined_input_for_agent_5 = (
                    f"### –°–ø–∏—Å–æ–∫ —Ç–µ–º/–≤–æ–ø—Ä–æ—Å–æ–≤ –∏–Ω—Ç–µ—Ä–≤—å—é:\n{agent_4_output}\n\n"
                    f"### –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é:\n{transcription_text}\n\n"
                    f"### CV –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:\n{cv_text}\n\n"
                    f"### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –≤–∞–∫–∞–Ω—Å–∏–∏:\n{requirements_text}\n\n"
                    f"### –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π:\n{matrix_text}\n\n"
                    f"### –¶–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞:\n{values_text}\n\n"
                    f"### –ü–æ—Ä—Ç—Ä–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:\n{portrait_text}"
                )

                logger.info("üöÄ –ó–∞–ø—É—Å–∫ agent_5_final_report_generator...")
                runner_5 = Runner(agent=agent_5_final_report_generator, app_name=settings.app_name,
                                  session_service=session_service)
                message_for_agent_5 = types.Content(role="user", parts=[types.Part(text=combined_input_for_agent_5)])
                agent_5_output = ""
                async for event in runner_5.run_async(session_id=session_id, user_id=user_id, new_message=message_for_agent_5):
                    if event.usage_metadata:
                        pipeline_tokens_used += event.usage_metadata.total_token_count
                        logger.info(f"–¢–æ–∫–µ–Ω—ã (–ê–≥–µ–Ω—Ç 5): –í—Ö–æ–¥={event.usage_metadata.prompt_token_count}, –í—ã—Ö–æ–¥={event.usage_metadata.candidates_token_count}, –í—Å–µ–≥–æ={event.usage_metadata.total_token_count}")
                    if event.content and event.content.parts:
                        agent_5_output += "".join(part.text for part in event.content.parts if part.text)

                self.session_total_tokens += pipeline_tokens_used
                logger.info(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –ü–∞–π–ø–ª–∞–π–Ω 2: {pipeline_tokens_used}")
                logger.info(f"–û–±—â–∏–π —Ä–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {self.session_total_tokens}")

                logger.info("–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON –æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞–≥–µ–Ω—Ç–∞...")
                try:
                    start_index_4 = agent_4_output.find('{')
                    end_index_4 = agent_4_output.rfind('}')
                    if start_index_4 != -1 and end_index_4 != -1:
                        clean_json_str_4 = agent_4_output[start_index_4:end_index_4 + 1]
                    else:
                        clean_json_str_4 = agent_4_output

                    topics_data = json.loads(clean_json_str_4)

                    start_index_5 = agent_5_output.find('{')
                    end_index_5 = agent_5_output.rfind('}')
                    if start_index_5 != -1 and end_index_5 != -1:
                        clean_json_str_5 = agent_5_output[start_index_5:end_index_5 + 1]
                    else:
                        clean_json_str_5 = agent_5_output

                    report_data = json.loads(clean_json_str_5)

                    if "topics" in topics_data and "interview_analysis" in report_data:
                        report_data["interview_analysis"]["topics"] = topics_data["topics"]

                    full_report = FullReport(**report_data)

                    return ResultsAnalysis(
                        message="Interview analysis completed successfully",
                        report=full_report
                    )
                except json.JSONDecodeError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
                    logger.error(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–π JSON –æ—Ç –ê–≥–µ–Ω—Ç–∞ 4: {agent_4_output}")
                    logger.error(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–π JSON –æ—Ç –ê–≥–µ–Ω—Ç–∞ 5: {agent_5_output}")
                    raise ValueError("AI-—Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö.")
            finally:
                # –≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è –≤—Å–µ–≥–¥–∞: –∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—Ö–∞, –∏ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏
                if temp_audio_path and os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
                    logger.info(f"–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {temp_audio_path} —É–¥–∞–ª–µ–Ω.")